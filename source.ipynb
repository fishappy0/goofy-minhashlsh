{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MinHashLSH**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In local system memory implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryMinHashLSH:\n",
    "    def __init__(self, documents, k=5):\n",
    "        self.documents = documents\n",
    "        self.shingles = None\n",
    "        self.signatures = None\n",
    "        self.buckets = None\n",
    "        self.num_bands = 8\n",
    "        self.k = k\n",
    "        self.num_perms = 128\n",
    "    \n",
    "    def shingling(self, documents=pd.DataFrame([\"\"]), k=5):\n",
    "        if documents.any().any() == \"\":\n",
    "            self.documents = documents\n",
    "\n",
    "        shingles = set()\n",
    "        doc_shingles = set()\n",
    "        for doc in self.documents[\"text\"]:\n",
    "            for i in range(len(doc) - k + 1):\n",
    "                shingle = doc[i:i+k]\n",
    "                shingles.add(shingle)\n",
    "                doc_shingles.add(shingle)\n",
    "        shingles = list(shingles)\n",
    "        \n",
    "        boolean_vectors = np.full((len(self.documents), len(shingles)), False, dtype=bool)\n",
    "        for i, doc in enumerate(self.documents[\"text\"]):\n",
    "            for j, shingle in enumerate(shingles):\n",
    "                if shingle in doc:\n",
    "                    boolean_vectors[i, j] = True\n",
    "        return pd.DataFrame(boolean_vectors, columns=shingles).transpose()\n",
    "        \n",
    "    def minhashing(self, shingles_bvs, num_perm=128):\n",
    "        signatures = []\n",
    "        for _ in range(0, num_perm):\n",
    "            hash_funcs = np.random.permutation(shingles_bvs.shape[0])\n",
    "            signature_row = []\n",
    "            for j in range(0, shingles_bvs.shape[1]):\n",
    "                for hash in hash_funcs:\n",
    "                    if shingles_bvs.iloc[hash, j]:\n",
    "                        signature_row.append(hash)\n",
    "                        break\n",
    "            signatures.append(signature_row)\n",
    "        return pd.DataFrame(signatures)\n",
    "    \n",
    "    def locality_sensitive_hashing(self, signatures, num_bands=8):\n",
    "        self.num_bands = num_bands\n",
    "        buckets = {}\n",
    "        for doc_id in signatures:\n",
    "            sig = signatures[doc_id]\n",
    "            for i in range(0, len(sig), self.num_bands):\n",
    "                band = hash(tuple(sig[i:i+self.num_bands]))\n",
    "                if band in buckets:\n",
    "                    buckets[band].add(doc_id)\n",
    "                else:\n",
    "                    buckets[band] = {doc_id}\n",
    "        return buckets\n",
    "\n",
    "    def run(self, **kwargs):\n",
    "        if \"documents\" in kwargs:\n",
    "            self.documents = kwargs[\"documents\"]\n",
    "        if \"bands\" in kwargs:\n",
    "            self.num_bands = kwargs[\"bands\"]\n",
    "        if \"num_perms\" in kwargs:\n",
    "            self.num_perms = kwargs[\"num_perms\"]\n",
    "        if \"k\" in kwargs:\n",
    "            self.k = kwargs[\"k\"]\n",
    "        bitvecs = self.shingling(self.documents, self.k)\n",
    "\n",
    "        # The regular permutation variant of the minhashing algorithm is used\n",
    "        # assuming that the amount of data process is relatively small to fit in memory\n",
    "        # of course we'll utilize the row hashing variant in the spark implementation\n",
    "        self.signatures = self.minhashing(bitvecs, self.num_perms) \n",
    "        self.buckets = self.locality_sensitive_hashing(self.signatures, self.num_bands)\n",
    "        return self.buckets\n",
    "    \n",
    "    def __jaccard_similarity(self, a, b):\n",
    "        return len(a & b) / len(a | b)\n",
    "    \n",
    "    def approximateNearestNeighbors(self, key, n):\n",
    "        n = 1 if n > 1 else n\n",
    "        sig = self.signatures[key]\n",
    "        similar_docs = {}\n",
    "        for i in range(0, len(sig), self.num_bands):\n",
    "            band_hash = hash(tuple(sig[i:i+self.num_bands]))\n",
    "            if band_hash in self.buckets:\n",
    "                for doc_id in self.buckets[band_hash]:\n",
    "                    if doc_id != key:\n",
    "                        if doc_id in similar_docs:\n",
    "                            similar_docs[doc_id] += 1\n",
    "                        else:\n",
    "                            similar_docs[doc_id] = 1\n",
    "        similar_docs = {k: v for k, v in sorted(similar_docs.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        most_similar_docs = []\n",
    "        for doc_id in similar_docs:\n",
    "            jac_sim = self.__jaccard_similarity(set(self.signatures[key]), set(self.signatures[doc_id]))\n",
    "            if jac_sim > n:\n",
    "                most_similar_docs.append((doc_id, jac_sim))\n",
    "        return most_similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1.0), (4, 0.6666666666666666), (3, 0.55), (1, 0.5483870967741935)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs = [\"This is a test document\", \"This document is another test document\", \"This is a test document\",\"This is a test\",\"This is a document\", \"Hello word\"]\n",
    "docs_df = pd.DataFrame(test_docs, columns=[\"text\"])\n",
    "in_memory_lsh = InMemoryMinHashLSH(docs_df)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# bool_vecs = in_memory_lsh.shingling()\n",
    "# sigs = in_memory_lsh.minhashing(bool_vecs, 128)\n",
    "# buckets = in_memory_lsh.locality_sensitive_hashing(sigs)\n",
    "buckets = in_memory_lsh.run(k=3, bands=4, num_perms=256, documents=docs_df)\n",
    "# print(buckets)\n",
    "in_memory_lsh.approximateNearestNeighbors(0, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql import window as W\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Function name: hash_gen\n",
    "# Input: (Int) the number of \"hash functions\" to generate (Also happen to be signature length)\n",
    "# Output: (List) of tuples of 3 containing the hash functions parameters, example [(1, 3, 5), (2, 4, 6), ...]\n",
    "# Purpose: Generate the hash functions parameters for the minhashing process. This function is part of the minhashing method\n",
    "###############\n",
    "def hash_gen(num_hashes=128):\n",
    "   hashes = []\n",
    "   for i in range(num_hashes):\n",
    "       hash = [np.random.randint(1, 1000), np.random.randint(1, 1000), np.random.randint(1, 1000)]\n",
    "       hashes.append(hash)\n",
    "   return hashes\n",
    "\n",
    "###############\n",
    "# Function name: hash_bands\n",
    "# Input: (List) of integers, (Int) the number of bands to hash the integers into (Defaulted to 2 if longer than signature length)\n",
    "# Output: (List) of integers of the hashed bands\n",
    "# Purpose: Hash the signature into bands for the locality sensitive hashing process. This function is part of the locality sensitive hashing method RDD\n",
    "###############\n",
    "def hash_bands(row, num_bands=8):\n",
    "    arr_size = int(len(row) / num_bands)\n",
    "    arr_size = 2 if arr_size == 0 else arr_size\n",
    "    bands = []\n",
    "    for i in range(0, len(row), arr_size):\n",
    "        bands.append(hash(tuple(row[i:i+arr_size])))\n",
    "    return bands\n",
    "\n",
    "###############\n",
    "# Function name: minhash_udf\n",
    "# Input: (List) of squashed tuples containing the row id and the boolean vector, (Int) the signature length\n",
    "# Output: (List) of lists containing the minhashed values\n",
    "# Purpose: Minhash the boolean vectors into signatures. This function is part of the minhashing method RDD\n",
    "###############\n",
    "def minhash_udf(row, hash_funcs):#, sig_length=128):\n",
    "    # Generating the \"infinite\" matrix (can't use np.inf as it's not supported by spark, class error w/e)\n",
    "    final_ans = [[-2 for _ in range(len(row[0][1]))] for _ in range(len(hash_funcs))]\n",
    "    for row in row:\n",
    "        row_id = row[0]\n",
    "        row_vals = row[1]\n",
    "        for i in range(len(hash_funcs)):\n",
    "            # Hashing the row id\n",
    "            curr_hash = ((hash_funcs[i][0] * row_id) + hash_funcs[i][1]) % hash_funcs[i][2]\n",
    "            for j in range(len(row_vals)):\n",
    "                if row_vals[j]:\n",
    "                    # Minhashing with the \"infinite\" matrix\n",
    "                    final_ans[i][j] = curr_hash if final_ans[i][j] == -2 else min(final_ans[i][j], curr_hash)\n",
    "    return final_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkMinHashLSH:\n",
    "    def __init__(self, documents, k=5):\n",
    "        self.documents = documents\n",
    "        self.signatures = None\n",
    "        self.num_bands = 8\n",
    "        self.k = k\n",
    "        self.num_perms = 128\n",
    "    \n",
    "    def shingling(self, **kwargs):\n",
    "        self.documents = kwargs[\"documents\"] if \"documents\" in kwargs else self.documents\n",
    "        \n",
    "        # High k values would result in document skipping if the document is smaller than the shingle size\n",
    "        shingle_size = 5 if \"k\" not in kwargs else kwargs[\"k\"]\n",
    "\n",
    "        # The monotonically increasing ID function only generates random unique IDs\n",
    "        # Thus requiring us to use the row_number function instead\n",
    "        # The following code is essentially creating a \"window\" of ordered from 1, the \"lit\"(literal) function acts the same way as passing an arg to a function \n",
    "        id_doc_df = self.documents.withColumn(\"docID\", F.row_number().over(W.Window.orderBy(F.lit(1))))\n",
    "        shingles = (id_doc_df.rdd.map(lambda x: (x[0],x[1])) \n",
    "                   .map(lambda x: list(set([(x[1], x[0][i:i+shingle_size]) for i in range(len(x[0]) - shingle_size + 1)]))) # Shingling with k size shingle\n",
    "                   .flatMap(lambda x: x) # DF formatting, from shape [(docID, shingle), ...] to (docID, shingle), aka unknown data shape to 2 columns\n",
    "                   .toDF([\"docID\", \"shingles\"])\n",
    "                    )\n",
    "                    \n",
    "        return (shingles.groupBy(\"shingles\")\n",
    "                .pivot(\"docID\") # Rotating the DF based on docIDs\n",
    "                .agg(F.lit(True)) # Create a true column and aggregate on each present intersected shingles corresponding to the docID\n",
    "                .fillna(False)) # Fill the NaNs left by the aggregation process with False\n",
    "    \n",
    "    def minhashing(self, **kwargs):\n",
    "        if \"bool_vecs\" not in kwargs:\n",
    "            raise ValueError(\"Boolean Vectors not provided\")\n",
    "        bool_vecs = kwargs[\"bool_vecs\"]\n",
    "        sig_length = 128 if \"sig_length\" not in kwargs else kwargs[\"sig_length\"]\n",
    "        \n",
    "        # Col numbering nothing exciting\n",
    "        bool_vecs = bool_vecs.withColumn(\"id\", F.row_number().over(W.Window.orderBy(F.lit(1)))) \n",
    "\n",
    "        # Grouping the bool vecs into lists for processing\n",
    "        zipped_bv = bool_vecs.select(\"id\", F.array([F.col(x) for x in bool_vecs.columns if x != \"id\" and x != \"shingles\"]).alias(\"vals\")) \n",
    "\n",
    "        # Defining the return type so it doesn't become string cuz spark doesnt know how to process lol   \n",
    "        typed_minhash_udf = F.udf(minhash_udf, ArrayType(ArrayType(IntegerType()))) \n",
    "        \n",
    "        # Generating the hash functions and transforming it into a spark udf friendly format for parameter passing\n",
    "        # This has to be done separately as spark reconstructs the table from scratch everytime show() is called \n",
    "        # or anything that requires the table to be \"materialized\", side effect of that is random signatures every time show is called\n",
    "        hash_funcs = hash_gen(sig_length)\n",
    "        transformed_hash_funcs = F.array([F.array(F.lit(x[0]), F.lit(x[1]), F.lit(x[2])) for x in hash_funcs])\n",
    "        \n",
    "        # Black magic, jk, continue reading at the above auxiliary udf func named \"minhash_udf\"\n",
    "        sigs = zipped_bv.agg(typed_minhash_udf(F.collect_list(F.struct(F.col(\"id\"), F.col(\"vals\"))), transformed_hash_funcs).alias(\"sigs\")) \n",
    "        \n",
    "        # Convert it to a more \"expected\" and \"familiar\" format of signatures per col, and renaming the columns\n",
    "        sig = (sigs\n",
    "                .select(F.explode(\"sigs\"))\n",
    "                .rdd\n",
    "                .flatMap(lambda x: x)\n",
    "                .toDF()\n",
    "                )\n",
    "        # Column renaming \n",
    "        return sig.select([F.col(x).alias(f\"{x.strip('_')}\") for x in sig.columns])\n",
    "    \n",
    "    def locality_sensitive_hashing(self, **kwargs):\n",
    "        if \"sigs\" not in kwargs:\n",
    "            raise ValueError(\"Signatures not provided\")\n",
    "        \n",
    "        num_bands = 10 if \"num_bands\" not in kwargs else kwargs[\"num_bands\"]\n",
    "        sigs = kwargs[\"sigs\"]\n",
    "\n",
    "        # Defining the aggregate expression, basically squashing the columns into lists\n",
    "        squash_sigs = [F.collect_list(F.col(x)).alias(x) for x in sigs.columns] \n",
    "        squashed_sigs = (sigs\n",
    "                        .agg(*squash_sigs)\n",
    "                        .select(F.explode(F.array([F.array(F.col(x)) for x in sigs.columns]))\n",
    "                       .alias(\"sigs\"))\n",
    "                       .withColumn(\"doc_id\", F.row_number().over(W.Window.orderBy(F.lit(1)))))\n",
    "        bands_list = squashed_sigs.rdd.map(lambda x: (x[1],hash_bands(x[0][0], num_bands)))\n",
    "        return  (bands_list\n",
    "                   .toDF([\"doc_id\", \"bands\"])\n",
    "                   .select(\"doc_id\", F.explode(\"bands\").alias(\"buckets\"))\n",
    "                   .groupBy(\"buckets\")\n",
    "                   .agg(F.collect_set(\"doc_id\").alias(\"doc_ids\")))\n",
    "\n",
    "    def run(self, **kwargs):\n",
    "        if \"documents\" in kwargs:\n",
    "            documents = kwargs[\"documents\"]\n",
    "        if \"bands\" in kwargs:\n",
    "            self.num_bands = kwargs[\"bands\"]\n",
    "        if \"num_perms\" in kwargs:\n",
    "            self.num_perms = kwargs[\"num_perms\"]\n",
    "        if \"k\" in kwargs:\n",
    "            self.k = kwargs[\"k\"]\n",
    "        bool_vecs = self.shingling(documents=documents, k=self.k)\n",
    "        signatures = self.minhashing(bool_vecs=bool_vecs, sig_length=self.num_perms)\n",
    "        self.signatures = signatures\n",
    "        buckets = self.locality_sensitive_hashing(sigs=signatures, num_bands=self.num_bands)\n",
    "        return buckets\n",
    "\n",
    "    def jaccard_similarity(self, first_key, second_key):\n",
    "        sig_table = self.signatures\n",
    "        union = sig_table.select(f\"{first_key}\").union(sig_table.select(f\"{second_key}\")).distinct().count()\n",
    "        intersect = sig_table.select(f\"{first_key}\").intersect(sig_table.select(f\"{second_key}\")).distinct().count()\n",
    "        return intersect / union\n",
    "    \n",
    "    def approximateNearestNeighbors(self, key, n,):\n",
    "        \n",
    "        num_bands = int(self.num_bands)\n",
    "        selected_sigs = (self.signatures\n",
    "            .select(f\"{key}\")\n",
    "            .agg(F.collect_list(f\"{key}\"))\n",
    "            .rdd.map(lambda x: hash_bands(x[0], num_bands))\n",
    "            .flatMap(lambda x: x)\n",
    "            .map(lambda x: (x, ))\n",
    "            .toDF([\"buckets\"])\n",
    "        )\n",
    "\n",
    "        similar_docs = buckets.join(selected_sigs, \"buckets\")\n",
    "        doc_id = similar_docs.select(F.explode(\"doc_ids\").alias(\"doc_id\")).distinct().collect()\n",
    "        similar_docs = []\n",
    "\n",
    "        # TODO: Implement threading for parallel processing\n",
    "        for doc in doc_id:\n",
    "            jac_sim = self.jaccard_similarity(key, doc.doc_id)\n",
    "            # if jac_sim > n:\n",
    "            similar_docs.append((doc.doc_id, jac_sim))\n",
    "        \n",
    "        return similar_docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"minHashLSH\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             buckets|  doc_ids|\n",
      "+--------------------+---------+\n",
      "| 2000638592450125738|   [5, 4]|\n",
      "| 1283277135472676147|      [4]|\n",
      "|-5545844477663614845|      [6]|\n",
      "|-5657247872280712689|      [2]|\n",
      "| 4307797553634642394|[5, 6, 4]|\n",
      "| 4803095124130366729|      [5]|\n",
      "| 2145580550687776204|   [1, 3]|\n",
      "|-3381654230262078061|   [5, 2]|\n",
      "|-6539622585183552344|      [2]|\n",
      "| 2422434102992159561|[1, 2, 3]|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 1.0), (3, 1.0), (2, 0.5555555555555556)]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"This is a test document\",), (\"This document is another test document\",), (\"This is a test document\",), (\"Hello wordello \",), (\"Word Hello world\",), (\"hello\", )]\n",
    "df = spark.createDataFrame(data, [\"text\"])\n",
    "spark_lsh = SparkMinHashLSH(df)\n",
    "# bool_vecs = spark_lsh.shingling(documents=df, k=2)\n",
    "# sigs = spark_lsh.minhashing(bool_vecs=bool_vecs, sig_length=512)\n",
    "# buckets = spark_lsh.locality_sensitive_hashing(sigs=sigs, num_bands=8)\n",
    "\n",
    "# Increase the number of bands and perms to decrease the sensitivity of the LSH, effectively reducing the number of false positives\n",
    "buckets = spark_lsh.run(documents=df,k=2, bands=8, num_perms=16, document=df)\n",
    "buckets.show(10)\n",
    "spark_lsh.approximateNearestNeighbors(1, 0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7380952380952381"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unions = int((spark_lsh.signatures.select(\"1\").union(spark_lsh.signatures.select(\"2\")).distinct().count()))\n",
    "# intersects = int(spark_lsh.signatures.select(\"1\").intersect(spark_lsh.signatures.select(\"2\")).distinct().count())\n",
    "# intersects/unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
