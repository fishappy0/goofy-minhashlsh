{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MinHashLSH**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In local system memory implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryMinHashLSH:\n",
    "    def __init__(self, documents, k=5):\n",
    "        self.documents = documents\n",
    "        self.shingles = None\n",
    "        self.signatures = None\n",
    "        self.buckets = None\n",
    "    \n",
    "    def shingling(self, documents=pd.DataFrame([\"\"]), k=5):\n",
    "        if documents.any().any() == \"\":\n",
    "            self.documents = documents\n",
    "\n",
    "        shingles = set()\n",
    "        doc_shingles = set()\n",
    "        for doc in self.documents[\"text\"]:\n",
    "            for i in range(len(doc) - k + 1):\n",
    "                shingle = doc[i:i+k]\n",
    "                shingles.add(shingle)\n",
    "                doc_shingles.add(shingle)\n",
    "        shingles = list(shingles)\n",
    "        \n",
    "        boolean_vectors = np.full((len(self.documents), len(shingles)), False, dtype=bool)\n",
    "        for i, doc in enumerate(self.documents[\"text\"]):\n",
    "            for j, shingle in enumerate(shingles):\n",
    "                if shingle in doc:\n",
    "                    boolean_vectors[i, j] = True\n",
    "        return pd.DataFrame(boolean_vectors, columns=shingles).transpose()\n",
    "        \n",
    "    def minhashing(self, shingles_bvs, num_perm=128):\n",
    "        signatures = []\n",
    "        for _ in range(0, num_perm):\n",
    "            hash_funcs = np.random.permutation(shingles_bvs.shape[0])\n",
    "            signature = []\n",
    "            for j in range(0, shingles_bvs.shape[1]):\n",
    "                for hash in hash_funcs:\n",
    "                    if shingles_bvs.iloc[hash, j]:\n",
    "                        signature.append(hash)\n",
    "                        break\n",
    "            signatures.append(signature)\n",
    "        return pd.DataFrame(signatures)\n",
    "    \n",
    "    def locality_sensitive_hashing(self, signature, num_bands=8, num_rows=16):\n",
    "        buckets = {}\n",
    "        for i in range(0, num_bands):\n",
    "            band = signature.iloc[i*num_rows:(i+1)*num_rows]\n",
    "            for j in range(0, band.shape[1]):\n",
    "                hashed_band = hash(tuple(band.iloc[:, j]))\n",
    "                if hashed_band in buckets:\n",
    "                    buckets[hashed_band].append(j)\n",
    "                else:\n",
    "                    buckets[hashed_band] = [j]\n",
    "        return buckets\n",
    "\n",
    "    def run(self, documents=\"\"):\n",
    "        if documents != \"\":\n",
    "            self.documents = documents\n",
    "        self.shingles = self.shingling(self.documents)\n",
    "        self.signatures = self.minhashing(self.shingles)\n",
    "        self.buckets = self.locality_sensitive_hashing(self.signatures)\n",
    "        return self.buckets\n",
    "    \n",
    "    def __jaccard_similarity(self, a, b):\n",
    "        return len(a & b) / len(a | b)\n",
    "    \n",
    "    def approximateNearestNeighbors(self, key, n):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1379140599642863165: [0, 2], 9040997445242030420: [1], -2011539986988155481: [3], 5927102016616960046: [0, 2], -7896971630727833918: [1], 4934936327892699789: [3], 244299711004002593: [0, 2], 7666908281629136108: [1], -9161432108047039538: [3], 8331251141106009057: [0, 2], 7309846408532718783: [1], -4169070300030851549: [3], 538062161962066744: [0, 2], -8568018872202180160: [1], -6376555837399552003: [3], 5925910462200784584: [0, 2], -4604407864863100061: [1], 6162459667513325389: [3], -2954536926530557813: [0, 2], 6257829669100608064: [1], 7776162716060853749: [3], 6853229475000010218: [0, 2], 6361141334233927601: [1], -8053711553699312521: [3]}\n"
     ]
    }
   ],
   "source": [
    "test_docs = [\"This is a test document\", \"This document is another test document\", \"This is a test document\", \"Hello word\"]\n",
    "docs_df = pd.DataFrame(test_docs, columns=[\"text\"])\n",
    "in_memory_lsh = InMemoryMinHashLSH(docs_df)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# bool_vecs = in_memory_lsh.shingling()\n",
    "# sigs = in_memory_lsh.minhashing(bool_vecs, 128)\n",
    "# buckets = in_memory_lsh.locality_sensitive_hashing(sigs)\n",
    "buckets = in_memory_lsh.run()\n",
    "print(buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql import window as W\n",
    "\n",
    "class SparkMinHashLSH:\n",
    "    def __init__(self, documents, k=5):\n",
    "        self.documents = documents\n",
    "\n",
    "    def shingling(self, **kwargs):\n",
    "        if \"documents\" in kwargs:\n",
    "            self.documents = kwargs[\"documents\"]\n",
    "        \n",
    "        # High k values would result in document skipping if the document is smaller than the shingle size\n",
    "        shingle_size = 5 if \"k\" not in kwargs else kwargs[\"k\"]\n",
    "\n",
    "        documents = self.documents.withColumn(\"docID\", F.row_number().over(W.Window.orderBy(F.lit(1))))\n",
    "        shingles = (documents.rdd.map(lambda x: (x[0],x[1]))\n",
    "                   .map(lambda x: list(set([(x[1], x[0][i:i+shingle_size]) for i in range(len(x[0]) - shingle_size + 1)])))\n",
    "                   .flatMap(lambda x: x)\n",
    "                   .toDF([\"docID\", \"shingles\"])\n",
    "                    )\n",
    "        return (shingles.groupBy(\"shingles\")\n",
    "                .pivot(\"docID\")\n",
    "                .agg(F.lit(True))\n",
    "                .fillna(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"minHashLSH\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-----+-----+-----+-----+\n",
      "|       shingles|    1|    2|    3|    4|    5|\n",
      "+---------------+-----+-----+-----+-----+-----+\n",
      "| another test d|false| true|false|false|false|\n",
      "|s a test docume| true|false| true|false|false|\n",
      "|s is a test doc| true|false| true|false|false|\n",
      "|This document i|false| true|false|false|false|\n",
      "|ument is anothe|false| true|false|false|false|\n",
      "|ther test docum|false| true|false|false|false|\n",
      "|t is another te|false| true|false|false|false|\n",
      "|This is a test | true|false| true|false|false|\n",
      "|s another test |false| true|false|false|false|\n",
      "|is another test|false| true|false|false|false|\n",
      "|nother test doc|false| true|false|false|false|\n",
      "|document is ano|false| true|false|false|false|\n",
      "|her test docume|false| true|false|false|false|\n",
      "|another test do|false| true|false|false|false|\n",
      "|ment is another|false| true|false|false|false|\n",
      "|is is a test do| true|false| true|false|false|\n",
      "|er test documen|false| true|false|false|false|\n",
      "|Hello wordello |false|false|false| true|false|\n",
      "|s document is a|false| true|false|false|false|\n",
      "| a test documen| true|false| true|false|false|\n",
      "|is a test docum| true|false| true|false|false|\n",
      "|a test document| true|false| true|false|false|\n",
      "|Word Hello worl|false|false|false|false| true|\n",
      "|cument is anoth|false| true|false|false|false|\n",
      "|ord Hello world|false|false|false|false| true|\n",
      "|r test document|false| true|false|false|false|\n",
      "|other test docu|false| true|false|false|false|\n",
      "|his document is|false| true|false|false|false|\n",
      "| document is an|false| true|false|false|false|\n",
      "|ent is another |false| true|false|false|false|\n",
      "|ocument is anot|false| true|false|false|false|\n",
      "|nt is another t|false| true|false|false|false|\n",
      "| is another tes|false| true|false|false|false|\n",
      "| is a test docu| true|false| true|false|false|\n",
      "|his is a test d| true|false| true|false|false|\n",
      "|is document is |false| true|false|false|false|\n",
      "+---------------+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"This is a test document\",), (\"This document is another test document\",), (\"This is a test document\",), (\"Hello wordello \",), (\"Word Hello world\",), (\"hello\", )]\n",
    "df = spark.createDataFrame(data, [\"text\"])\n",
    "spark_lsh = SparkMinHashLSH(df)\n",
    "spark_lsh.shingling(k=15).show(300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
