{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MinHashLSH**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In local system memory implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryMinHashLSH:\n",
    "    def __init__(self, documents, k=5):\n",
    "        self.documents = documents\n",
    "        self.shingles = None\n",
    "        self.signatures = None\n",
    "        self.buckets = None\n",
    "        self.num_bands = 8\n",
    "        self.k = k\n",
    "        self.num_perms = 128\n",
    "    \n",
    "    def shingling(self, documents=pd.DataFrame([\"\"]), k=5):\n",
    "        if documents.any().any() == \"\":\n",
    "            self.documents = documents\n",
    "\n",
    "        shingles = set()\n",
    "        doc_shingles = set()\n",
    "        for doc in self.documents[\"text\"]:\n",
    "            for i in range(len(doc) - k + 1):\n",
    "                shingle = doc[i:i+k]\n",
    "                shingles.add(shingle)\n",
    "                doc_shingles.add(shingle)\n",
    "        shingles = list(shingles)\n",
    "        \n",
    "        boolean_vectors = np.full((len(self.documents), len(shingles)), False, dtype=bool)\n",
    "        for i, doc in enumerate(self.documents[\"text\"]):\n",
    "            for j, shingle in enumerate(shingles):\n",
    "                if shingle in doc:\n",
    "                    boolean_vectors[i, j] = True\n",
    "        return pd.DataFrame(boolean_vectors, columns=shingles).transpose()\n",
    "        \n",
    "    def minhashing(self, shingles_bvs, num_perm=128):\n",
    "        signatures = []\n",
    "        for _ in range(0, num_perm):\n",
    "            hash_funcs = np.random.permutation(shingles_bvs.shape[0])\n",
    "            signature_row = []\n",
    "            for j in range(0, shingles_bvs.shape[1]):\n",
    "                for hash in hash_funcs:\n",
    "                    if shingles_bvs.iloc[hash, j]:\n",
    "                        signature_row.append(hash)\n",
    "                        break\n",
    "            signatures.append(signature_row)\n",
    "        return pd.DataFrame(signatures)\n",
    "    \n",
    "    def locality_sensitive_hashing(self, signatures, num_bands=8):\n",
    "        self.num_bands = num_bands\n",
    "        buckets = {}\n",
    "        for doc_id in signatures:\n",
    "            sig = signatures[doc_id]\n",
    "            for i in range(0, len(sig), self.num_bands):\n",
    "                band = hash(tuple(sig[i:i+self.num_bands]))\n",
    "                if band in buckets:\n",
    "                    buckets[band].add(doc_id)\n",
    "                else:\n",
    "                    buckets[band] = {doc_id}\n",
    "        return buckets\n",
    "\n",
    "    def run(self, **kwargs):\n",
    "        if \"documents\" in kwargs:\n",
    "            self.documents = kwargs[\"documents\"]\n",
    "        if \"bands\" in kwargs:\n",
    "            self.num_bands = kwargs[\"bands\"]\n",
    "        if \"num_perms\" in kwargs:\n",
    "            self.num_perms = kwargs[\"num_perms\"]\n",
    "        if \"k\" in kwargs:\n",
    "            self.k = kwargs[\"k\"]\n",
    "        bitvecs = self.shingling(self.documents, self.k)\n",
    "\n",
    "        # The regular permutation variant of the minhashing algorithm is used\n",
    "        # assuming that the amount of data process is relatively small to fit in memory\n",
    "        # of course we'll utilize the row hashing variant in the spark implementation\n",
    "        self.signatures = self.minhashing(bitvecs, self.num_perms) \n",
    "        self.buckets = self.locality_sensitive_hashing(self.signatures, self.num_bands)\n",
    "        return self.buckets\n",
    "    \n",
    "    def __jaccard_similarity(self, a, b):\n",
    "        return len(a & b) / len(a | b)\n",
    "    \n",
    "    def approximateNearestNeighbors(self, key, n):\n",
    "        n = 1 if n > 1 else n\n",
    "        sig = self.signatures[key]\n",
    "        similar_docs = {}\n",
    "        for i in range(0, len(sig), self.num_bands):\n",
    "            band_hash = hash(tuple(sig[i:i+self.num_bands]))\n",
    "            if band_hash in self.buckets:\n",
    "                for doc_id in self.buckets[band_hash]:\n",
    "                    if doc_id != key:\n",
    "                        if doc_id in similar_docs:\n",
    "                            similar_docs[doc_id] += 1\n",
    "                        else:\n",
    "                            similar_docs[doc_id] = 1\n",
    "        similar_docs = {k: v for k, v in sorted(similar_docs.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        most_similar_docs = []\n",
    "        for doc_id in similar_docs:\n",
    "            jac_sim = self.__jaccard_similarity(set(self.signatures[key]), set(self.signatures[doc_id]))\n",
    "            if jac_sim > n:\n",
    "                most_similar_docs.append((doc_id, jac_sim))\n",
    "        return most_similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1.0), (4, 0.6666666666666666), (3, 0.55), (1, 0.5483870967741935)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs = [\"This is a test document\", \"This document is another test document\", \"This is a test document\",\"This is a test\",\"This is a document\", \"Hello word\"]\n",
    "docs_df = pd.DataFrame(test_docs, columns=[\"text\"])\n",
    "in_memory_lsh = InMemoryMinHashLSH(docs_df)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# bool_vecs = in_memory_lsh.shingling()\n",
    "# sigs = in_memory_lsh.minhashing(bool_vecs, 128)\n",
    "# buckets = in_memory_lsh.locality_sensitive_hashing(sigs)\n",
    "buckets = in_memory_lsh.run(k=3, bands=4, num_perms=256, documents=docs_df)\n",
    "# print(buckets)\n",
    "in_memory_lsh.approximateNearestNeighbors(0, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql import window as W\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "\n",
    "def hash_gen(num_hashes=128):\n",
    "   hashes = []\n",
    "   for i in range(num_hashes):\n",
    "       hash = (np.random.randint(1, 1000), np.random.randint(1, 1000), np.random.randint(1, 1000))\n",
    "       hashes.append(hash)\n",
    "   return hashes\n",
    "\n",
    "# This follows the row hashing algorithm with the input as a squashed list of the lists of the boolean vectors\n",
    "def minhash_udf(row, sig_length=128):\n",
    "\n",
    "    # Generating the fixed parameters for the hash functions\n",
    "    hash_funcs = hash_gen(sig_length)\n",
    "    \n",
    "    # Generating the \"infinite\" matrix (can't use np.inf as it's not supported by spark, class error w/e)\n",
    "    final_ans = [[-2 for _ in range(len(row[0][1]))] for _ in range(len(hash_funcs))]\n",
    "    for row in row:\n",
    "        row_id = row[0]\n",
    "        row_vals = row[1]\n",
    "        for i in range(len(hash_funcs)):\n",
    "            # Hashing the row id\n",
    "            curr_hash = ((hash_funcs[i][0] * row_id) + hash_funcs[i][1]) % hash_funcs[i][2]\n",
    "            for j in range(len(row_vals)):\n",
    "                if row_vals[j]:\n",
    "                    # Minhashing with the \"infinite\" matrix\n",
    "                    final_ans[i][j] = curr_hash if final_ans[i][j] == -2 else min(final_ans[i][j], curr_hash)\n",
    "    return final_ans\n",
    "\n",
    "class SparkMinHashLSH:\n",
    "    def __init__(self, documents, k=5):\n",
    "        self.documents = documents\n",
    "    \n",
    "    \n",
    "    def shingling(self, **kwargs):\n",
    "        if \"documents\" in kwargs:\n",
    "            self.documents = kwargs[\"documents\"]\n",
    "        \n",
    "        # High k values would result in document skipping if the document is smaller than the shingle size\n",
    "        shingle_size = 5 if \"k\" not in kwargs else kwargs[\"k\"]\n",
    "        # The monotonically increasing ID function only generates random unique IDs\n",
    "        # Thus requiring us to use the row_number function instead\n",
    "        # The following code is essentially creating a \"window\" of ordered from 1, the \"lit\"(literal) function acts the same way as passing an arg to a function \n",
    "        documents = self.documents.withColumn(\"docID\", F.row_number().over(W.Window.orderBy(F.lit(1))))\n",
    "        shingles = (documents.rdd.map(lambda x: (x[0],x[1])) \n",
    "                   .map(lambda x: list(set([(x[1], x[0][i:i+shingle_size]) for i in range(len(x[0]) - shingle_size + 1)]))) # Shingling with k size shingle\n",
    "                   .flatMap(lambda x: x) # DF formatting, from shape [(docID, shingle), ...] to (docID, shingle), aka unknown data shape to 2 columns\n",
    "                   .toDF([\"docID\", \"shingles\"])\n",
    "                    )\n",
    "        return (shingles.groupBy(\"shingles\")\n",
    "                .pivot(\"docID\") # Rotating the DF based on docIDs\n",
    "                .agg(F.lit(True)) # Create a true column and aggregate on each present intersected shingles corresponding to the docID\n",
    "                .fillna(False)) # Fill the NaNs left by the aggregation process with False\n",
    "    \n",
    "    def minhashing(self, sc, **kwargs):\n",
    "        if \"bool_vecs\" not in kwargs:\n",
    "            raise ValueError(\"Boolean Vectors not provided\")\n",
    "        bool_vecs = kwargs[\"bool_vecs\"]\n",
    "        sig_length = 128 if \"sig_length\" not in kwargs else kwargs[\"sig_length\"]\n",
    "        \n",
    "        bool_vecs = bool_vecs.withColumn(\"id\", F.row_number().over(W.Window.orderBy(F.lit(1)))) # Col numbering nothing exciting\n",
    "\n",
    "        # Grouping the bool vecs into lists for processing\n",
    "        bool_vecs = bool_vecs.select(\"id\", F.array([F.col(x) for x in bool_vecs.columns if x != \"id\" and x != \"shingles\"]).alias(\"vals\")) \n",
    "        typed_minhash_udf = F.udf(minhash_udf, ArrayType(ArrayType(IntegerType()))) # Defining the return type so it doesn't become string cuz spark doesnt know how to process lol   \n",
    "        sigs = bool_vecs.agg(typed_minhash_udf(F.collect_list(F.struct(F.col(\"id\"), F.col(\"vals\"))), F.lit(sig_length)).alias(\"sigs\")) # Black magic, jk, continue reading at the above udf func\n",
    "\n",
    "        return sigs.select(F.explode(\"sigs\")).rdd.flatMap(lambda x: x).toDF() # Convert it to a more \"expected\" and \"familiar\" format of signatures per col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"minHashLSH\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+\n",
      "| _1| _2| _3| _4| _5| _6|\n",
      "+---+---+---+---+---+---+\n",
      "| 10| 10| 10| 55| 55| 27|\n",
      "|171|171|171|171|171|171|\n",
      "| 25| 25| 25|  5| 65|184|\n",
      "| 23| 22| 23| 83| 83| 22|\n",
      "| 19| 19| 19|  3|  3| 75|\n",
      "| 24| 24| 24| 30|  0| 30|\n",
      "| 23| 23| 23|197|197|197|\n",
      "|  1|  1|  1| 19|  0|106|\n",
      "|  8|  8|  8| 80| 80| 26|\n",
      "| 40| 40| 40| 65| 60|199|\n",
      "|  3|  3|  3|331| 57|536|\n",
      "| 19|  9| 19| 81| 51|193|\n",
      "|145|145|145|150|144|152|\n",
      "| 14| 14| 14| 58|  6| 58|\n",
      "|  2|  2|  2|  0|  0|  0|\n",
      "| 34| 34| 34|252|  8|418|\n",
      "+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"This is a test document\",), (\"This document is another test document\",), (\"This is a test document\",), (\"Hello wordello \",), (\"Word Hello world\",), (\"hello\", )]\n",
    "df = spark.createDataFrame(data, [\"text\"])\n",
    "spark_lsh = SparkMinHashLSH(df)\n",
    "bool_vecs = spark_lsh.shingling(k=2)\n",
    "sigs = spark_lsh.minhashing(sc, bool_vecs=bool_vecs, sig_length=16)\n",
    "sigs.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
