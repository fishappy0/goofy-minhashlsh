{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MinHashLSH**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In local system memory implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryMinHashLSH:\n",
    "    def __init__(self, documents, k=5):\n",
    "        self.documents = documents\n",
    "        self.shingles = None\n",
    "        self.signatures = None\n",
    "        self.buckets = None\n",
    "        self.num_bands = 8\n",
    "        self.k = k\n",
    "        self.num_perms = 128\n",
    "    \n",
    "    def shingling(self, documents=pd.DataFrame([\"\"]), k=5):\n",
    "        if documents.any().any() == \"\":\n",
    "            self.documents = documents\n",
    "\n",
    "        shingles = set()\n",
    "        doc_shingles = set()\n",
    "        for doc in self.documents[\"text\"]:\n",
    "            for i in range(len(doc) - k + 1):\n",
    "                shingle = doc[i:i+k]\n",
    "                shingles.add(shingle)\n",
    "                doc_shingles.add(shingle)\n",
    "        shingles = list(shingles)\n",
    "        \n",
    "        boolean_vectors = np.full((len(self.documents), len(shingles)), False, dtype=bool)\n",
    "        for i, doc in enumerate(self.documents[\"text\"]):\n",
    "            for j, shingle in enumerate(shingles):\n",
    "                if shingle in doc:\n",
    "                    boolean_vectors[i, j] = True\n",
    "        return pd.DataFrame(boolean_vectors, columns=shingles).transpose()\n",
    "        \n",
    "    def minhashing(self, shingles_bvs, num_perm=128):\n",
    "        signatures = []\n",
    "        for _ in range(0, num_perm):\n",
    "            hash_funcs = np.random.permutation(shingles_bvs.shape[0])\n",
    "            signature_row = []\n",
    "            for j in range(0, shingles_bvs.shape[1]):\n",
    "                for hash in hash_funcs:\n",
    "                    if shingles_bvs.iloc[hash, j]:\n",
    "                        signature_row.append(hash)\n",
    "                        break\n",
    "            signatures.append(signature_row)\n",
    "        return pd.DataFrame(signatures)\n",
    "    \n",
    "    def locality_sensitive_hashing(self, signatures, num_bands=8):\n",
    "        self.num_bands = num_bands\n",
    "        buckets = {}\n",
    "        for doc_id in signatures:\n",
    "            sig = signatures[doc_id]\n",
    "            for i in range(0, len(sig), self.num_bands):\n",
    "                band = hash(tuple(sig[i:i+self.num_bands]))\n",
    "                if band in buckets:\n",
    "                    buckets[band].add(doc_id)\n",
    "                else:\n",
    "                    buckets[band] = {doc_id}\n",
    "        return buckets\n",
    "\n",
    "    def run(self, **kwargs):\n",
    "        if \"documents\" in kwargs:\n",
    "            self.documents = kwargs[\"documents\"]\n",
    "        if \"bands\" in kwargs:\n",
    "            self.num_bands = kwargs[\"bands\"]\n",
    "        if \"num_perms\" in kwargs:\n",
    "            self.num_perms = kwargs[\"num_perms\"]\n",
    "        if \"k\" in kwargs:\n",
    "            self.k = kwargs[\"k\"]\n",
    "        bitvecs = self.shingling(self.documents, self.k)\n",
    "\n",
    "        # The regular permutation variant of the minhashing algorithm is used\n",
    "        # assuming that the amount of data process is relatively small to fit in memory\n",
    "        # of course we'll utilize the row hashing variant in the spark implementation\n",
    "        self.signatures = self.minhashing(bitvecs, self.num_perms) \n",
    "        self.buckets = self.locality_sensitive_hashing(self.signatures, self.num_bands)\n",
    "        return self.buckets\n",
    "    \n",
    "    def __jaccard_similarity(self, a, b):\n",
    "        return len(a & b) / len(a | b)\n",
    "    \n",
    "    def approximateNearestNeighbors(self, key, n):\n",
    "        n = 1 if n > 1 else n\n",
    "        sig = self.signatures[key]\n",
    "        similar_docs = {}\n",
    "        for i in range(0, len(sig), self.num_bands):\n",
    "            band_hash = hash(tuple(sig[i:i+self.num_bands]))\n",
    "            if band_hash in self.buckets:\n",
    "                for doc_id in self.buckets[band_hash]:\n",
    "                    if doc_id != key:\n",
    "                        if doc_id in similar_docs:\n",
    "                            similar_docs[doc_id] += 1\n",
    "                        else:\n",
    "                            similar_docs[doc_id] = 1\n",
    "        similar_docs = {k: v for k, v in sorted(similar_docs.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        most_similar_docs = []\n",
    "        for doc_id in similar_docs:\n",
    "            jac_sim = self.__jaccard_similarity(set(self.signatures[key]), set(self.signatures[doc_id]))\n",
    "            if jac_sim > n:\n",
    "                most_similar_docs.append((doc_id, jac_sim))\n",
    "        return most_similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1.0), (4, 0.6666666666666666), (1, 0.5483870967741935), (3, 0.55)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs = [\"This is a test document\", \"This document is another test document\", \"This is a test document\",\"This is a test\",\"This is a document\", \"Hello word\"]\n",
    "docs_df = pd.DataFrame(test_docs, columns=[\"text\"])\n",
    "in_memory_lsh = InMemoryMinHashLSH(docs_df)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# bool_vecs = in_memory_lsh.shingling()\n",
    "# sigs = in_memory_lsh.minhashing(bool_vecs, 128)\n",
    "# buckets = in_memory_lsh.locality_sensitive_hashing(sigs)\n",
    "buckets = in_memory_lsh.run(k=3, bands=4, num_perms=256, documents=docs_df)\n",
    "# print(buckets)\n",
    "in_memory_lsh.approximateNearestNeighbors(0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fisha\\AppData\\Local\\Temp\\ipykernel_15356\\2916449186.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  wos_5736 = pd.read_csv(\"WebOfScience-5736.txt\", sep=\"\\r\\n\", header=None, names=[\"text\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wos_5736 = pd.read_csv(\"WebOfScience-5736.txt\", sep=\"\\r\\n\", header=None, names=[\"text\"])\n",
    "buckets = in_memory_lsh.run(k=16, bands=8, num_perms=256, documents=wos_5736[:100])\n",
    "in_memory_lsh.approximateNearestNeighbors(59, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql import window as W\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "###############\n",
    "# Function name: hash_gen\n",
    "# Input: (Int) the number of \"hash functions\" to generate (Also happen to be signature length)\n",
    "# Output: (List) of tuples of 3 containing the hash functions parameters, example [(1, 3, 5), (2, 4, 6), ...]\n",
    "# Purpose: Generate the hash functions parameters for the minhashing process. This function is part of the minhashing method\n",
    "###############\n",
    "def hash_gen(num_hashes=128):\n",
    "   hashes = []\n",
    "   for i in range(num_hashes):\n",
    "       hash = [np.random.randint(1, 1000), np.random.randint(1, 1000), np.random.randint(1, 1000)]\n",
    "       hashes.append(hash)\n",
    "   return hashes\n",
    "\n",
    "###############\n",
    "# Function name: hash_bands\n",
    "# Input: (List) of integers, (Int) the number of bands to hash the integers into (Defaulted to 2 if longer than signature length)\n",
    "# Output: (List) of integers of the hashed bands\n",
    "# Purpose: Hash the signature into bands for the locality sensitive hashing process. This function is part of the locality sensitive hashing method RDD\n",
    "###############\n",
    "def hash_bands(row, num_bands=8):\n",
    "    arr_size = int(len(row) / num_bands)\n",
    "    arr_size = 2 if arr_size == 0 else arr_size\n",
    "    bands = []\n",
    "    for i in range(0, len(row), arr_size):\n",
    "        bands.append(hash(tuple(row[i:i+arr_size])))\n",
    "    return bands\n",
    "\n",
    "###############\n",
    "# Function name: minhash_udf\n",
    "# Input: (List) of squashed tuples containing the row id and the boolean vector, (Int) the signature length\n",
    "# Output: (List) of lists containing the minhashed values\n",
    "# Purpose: Minhash the boolean vectors into signatures. This function is part of the minhashing method RDD\n",
    "###############\n",
    "def minhash_udf(row, hash_funcs):#, sig_length=128):\n",
    "    hf_len = int(len(hash_funcs))\n",
    "    # Generating the \"infinite\" matrix (can't use np.inf as it's not supported by spark, class error w/e)\n",
    "    final_ans = [[-2 for _ in range(len(row[0][1]))] for _ in range(hf_len)]\n",
    "    for row in row:\n",
    "        row_id = row[0]\n",
    "        row_vals = row[1]\n",
    "        for i in range(hf_len):\n",
    "            # Hashing the row id\n",
    "            curr_hash = ((hash_funcs[i][0] * row_id) + hash_funcs[i][1]) % hash_funcs[i][2]\n",
    "            for j in range(len(row_vals)):\n",
    "                if row_vals[j]:\n",
    "                    # Minhashing with the \"infinite\" matrix\n",
    "                    final_ans[i][j] = curr_hash if final_ans[i][j] == -2 else min(final_ans[i][j], curr_hash)\n",
    "    return final_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkMinHashLSH:\n",
    "    def __init__(self, documents, k=5):\n",
    "        self.documents = documents\n",
    "        self.bool_vecs = None\n",
    "        self.signatures = None\n",
    "        self.buckets = None\n",
    "        self.num_bands = 8\n",
    "        self.k = k\n",
    "        self.num_perms = 128\n",
    "    \n",
    "    ###############\n",
    "    # Function name: shingling\n",
    "    # Input: kwargs: (DataFrame) documents=, (Int) k=\n",
    "    # Output: (DataFrame) of the shingled documents represented as boolean vectors\n",
    "    #         Sample output:\n",
    "    #           +-------------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
    "    #           |shingles     |1        |2        |3        |4        |5        |6        |7        |8        |\n",
    "    #           +-------------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
    "    #           |this         |True     |True     |True     |True     |True     |True     |True     |True     |\n",
    "    #           |is a         |True     |False    |False    |True     |False    |True     |False    |True     |\n",
    "    #           |test         |False    |True     |False    |True     |False    |True     |True     |True     |\n",
    "    #           |document     |True     |False    |True     |False    |True     |True     |True     |False    |\n",
    "    #           +-------------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
    "    # Purpose: Shingle the documents into k size shingles. This function is part of the minhashing method RDD\n",
    "    ###############\n",
    "    def shingling(self, **kwargs):\n",
    "        self.documents = kwargs[\"documents\"] if \"documents\" in kwargs else self.documents\n",
    "        \n",
    "        # High k values would result in document skipping if the document is smaller than the shingle size\n",
    "        shingle_size = 5 if \"k\" not in kwargs else kwargs[\"k\"]\n",
    "\n",
    "        # The monotonically increasing ID function only generates random unique IDs\n",
    "        # Thus requiring us to use the row_number function instead\n",
    "        # The following code is essentially creating an ordered \"window\"  from 1, the \"lit\"(literal) function acts the same way as passing an arg to a function \n",
    "        id_doc_df = self.documents.withColumn(\"docID\", F.row_number().over(W.Window.orderBy(F.lit(1))))\n",
    "        shingles = (id_doc_df.rdd.map(lambda x: (x[0],x[1])) \n",
    "                   .map(lambda x: list(set([(x[1], x[0][i:i+shingle_size]) for i in range(len(x[0]) - shingle_size + 1)]))) # Shingling with k size shingle\n",
    "                   .flatMap(lambda x: x) # DF formatting, from shape [(docID, shingle), ...] to (docID, shingle), aka unknown data shape to 2 columns\n",
    "                   .toDF([\"docID\", \"shingles\"])\n",
    "                    )\n",
    "                    \n",
    "        ret_bv = (shingles.groupBy(\"shingles\")\n",
    "                .pivot(\"docID\") # Rotating the DF based on docIDs\n",
    "                .agg(F.lit(True)) # Create a true column and aggregate on each present intersected shingles corresponding to the docID\n",
    "                .fillna(False)) # Fill the NaNs left by the aggregation process with False\n",
    "        self.bool_vecs = ret_bv\n",
    "        return ret_bv\n",
    "\n",
    "    ###############\n",
    "    # Function name: minhashing\n",
    "    # Input: kwargs: (DataFrame) bool_vecs=, (Int) sig_length=\n",
    "    # Output: (DataFrame) of the minhashed signatures\n",
    "    #         Sample output:\n",
    "    #         +----+----+----+----+----+----+----+----+\n",
    "    #         |1   |2   |3   |4   |5   |6   |7   |8   |\n",
    "    #         +----+----+----+----+----+----+----+----+\n",
    "    #         |1   |2   |3   |4   |5   |6   |7   |8   |\n",
    "    #         |2   |3   |4   |5   |6   |7   |8   |9   |\n",
    "    #         +----+----+----+----+----+----+----+----+\n",
    "    # Purpose: Minhash the boolean vectors into signatures (A form of embedding), used for the LSH process. \n",
    "    ###############\n",
    "    def minhashing(self, **kwargs):\n",
    "        if \"bool_vecs\" not in kwargs:\n",
    "            raise ValueError(\"Boolean Vectors not provided\")\n",
    "        bool_vecs = kwargs[\"bool_vecs\"]\n",
    "        sig_length = 128 if \"sig_length\" not in kwargs else kwargs[\"sig_length\"]\n",
    "        \n",
    "        # Col numbering nothing exciting\n",
    "        bool_vecs = bool_vecs.withColumn(\"id\", F.row_number().over(W.Window.orderBy(F.lit(1)))) \n",
    "\n",
    "        # Grouping the bool vecs into lists for processing\n",
    "        zipped_bv = bool_vecs.select(\"id\", F.array([F.col(x) for x in bool_vecs.columns if x != \"id\" and x != \"shingles\"]).alias(\"vals\")) \n",
    "\n",
    "        # Defining the return type so it doesn't become string cuz spark doesnt know how to process lol   \n",
    "        typed_minhash_udf = F.udf(minhash_udf, ArrayType(ArrayType(IntegerType()))) \n",
    "        \n",
    "        # Generating the hash functions and transforming it into a spark udf friendly format for parameter passing\n",
    "        # This has to be done separately as spark reconstructs the table from scratch everytime show() is called \n",
    "        # or anything that requires the table to be \"materialized\", side effect of that is random signatures every time show is called\n",
    "        hash_funcs = hash_gen(sig_length)\n",
    "        transformed_hash_funcs = F.array([F.array(F.lit(x[0]), F.lit(x[1]), F.lit(x[2])) for x in hash_funcs])\n",
    "        \n",
    "        # Black magic, jk, continue reading at the above auxiliary udf func named \"minhash_udf\"\n",
    "        sigs = zipped_bv.agg(typed_minhash_udf(F.collect_list(F.struct(F.col(\"id\"), F.col(\"vals\"))), transformed_hash_funcs).alias(\"sigs\")) \n",
    "\n",
    "        # Convert it to a more \"expected\" and \"familiar\" format of signatures per col, and renaming the columns\n",
    "        sig = (sigs\n",
    "                .select(F.explode(\"sigs\"))\n",
    "                .rdd\n",
    "                .flatMap(lambda x: x)\n",
    "                .toDF()\n",
    "                )\n",
    "        # Column renaming \n",
    "        ret_sig = sig.select([F.col(x).alias(f\"{x.strip('_')}\") for x in sig.columns])\n",
    "        self.signatures = ret_sig\n",
    "        return ret_sig\n",
    "    \n",
    "    ###############\n",
    "    # Function name: locality_sensitive_hashing\n",
    "    # Input: kwargs: (DataFrame) sigs=, (Int) num_bands=\n",
    "    # Output: (DataFrame) of the hashed buckets\n",
    "    #         Sample output:\n",
    "    #         +------------------+----------+\n",
    "    #         |buckets           |doc_ids   |\n",
    "    #         +------------------+----------+\n",
    "    #         |19248921412094    |[1, 2, 3] |\n",
    "    #         |46948691412095    |   [2, 4] |\n",
    "    #         |69420911123456    |      [3] |\n",
    "    #         +------------------+----------+\n",
    "    # Purpose: Hash the signatures into buckets, which then can be used to find similar documents, by rehashing the query document and finding the intersecting buckets\n",
    "    ###############\n",
    "    def locality_sensitive_hashing(self, **kwargs):\n",
    "        if \"sigs\" not in kwargs:\n",
    "            raise ValueError(\"Signatures not provided\")\n",
    "        \n",
    "        num_bands = 10 if \"num_bands\" not in kwargs else kwargs[\"num_bands\"]\n",
    "        sigs = kwargs[\"sigs\"]\n",
    "\n",
    "        # Defining the aggregate expression, basically squashing the columns into lists\n",
    "        squash_sigs = [F.collect_list(F.col(x)).alias(x) for x in sigs.columns] \n",
    "        squashed_sigs = (sigs\n",
    "                        .agg(*squash_sigs)\n",
    "                        .select(F.explode(F.array([F.array(F.col(x)) for x in sigs.columns]))\n",
    "                       .alias(\"sigs\"))\n",
    "                       .withColumn(\"doc_id\", F.row_number().over(W.Window.orderBy(F.lit(1)))))\n",
    "        bands_list = squashed_sigs.rdd.map(lambda x: (x[1],hash_bands(x[0][0], num_bands)))\n",
    "        ret_buckets =   (bands_list\n",
    "                   .toDF([\"doc_id\", \"bands\"])\n",
    "                   .select(\"doc_id\", F.explode(\"bands\").alias(\"buckets\"))\n",
    "                   .groupBy(\"buckets\")\n",
    "                   .agg(F.collect_set(\"doc_id\").alias(\"doc_ids\")))\n",
    "        self.buckets = ret_buckets\n",
    "        return ret_buckets\n",
    "    \n",
    "    ###############\n",
    "    # Function name: run\n",
    "    # Input: kwargs: (DataFrame) documents=, (Int) k=, (Int) bands=, (Int) num_perms=\n",
    "    # Output: (DataFrame) of the hashed buckets, output of the locality sensitive hashing function\n",
    "    # Purpose: The main function to run the entire process of minhashing and locality sensitive hashing\n",
    "    ###############\n",
    "    def run(self, **kwargs):\n",
    "        if \"documents\" in kwargs:\n",
    "            documents = kwargs[\"documents\"]\n",
    "        if \"bands\" in kwargs:\n",
    "            self.num_bands = kwargs[\"bands\"]\n",
    "        if \"num_perms\" in kwargs:\n",
    "            self.num_perms = kwargs[\"num_perms\"]\n",
    "        if \"k\" in kwargs:\n",
    "            self.k = kwargs[\"k\"]\n",
    "        t_start = time.time()\n",
    "        bool_vecs = self.shingling(documents=documents, k=self.k)\n",
    "        t_stop = time.time()\n",
    "        print(f\"Shingling took {t_stop - t_start} seconds\")\n",
    "\n",
    "        t_start = time.time()\n",
    "        signatures = self.minhashing(bool_vecs=bool_vecs, sig_length=self.num_perms)\n",
    "        t_stop = time.time()\n",
    "        print(f\"Minhashing took {t_stop - t_start} seconds\")\n",
    "\n",
    "        self.signatures = signatures\n",
    "        t_start = time.time()\n",
    "        buckets = self.locality_sensitive_hashing(sigs=signatures, num_bands=self.num_bands)\n",
    "        t_stop = time.time()\n",
    "        print(f\"Locality Sensitive Hashing took {t_stop - t_start} seconds\")\n",
    "        return buckets\n",
    "\n",
    "    ###############\n",
    "    # Function name: jaccard_similarity\n",
    "    # Input: (String) first_key, (String) second_key\n",
    "    # Output: (Float) of the jaccard similarity between the two documents\n",
    "    # Purpose: Calculate the jaccard similarity between two documents columns\n",
    "    ###############\n",
    "    def jaccard_similarity(self, first_key, second_key):\n",
    "        sig_table = self.signatures\n",
    "        union = sig_table.select(f\"{first_key}\").union(sig_table.select(f\"{second_key}\")).distinct().count()\n",
    "        intersect = sig_table.select(f\"{first_key}\").intersect(sig_table.select(f\"{second_key}\")).distinct().count()\n",
    "        return intersect / union\n",
    "    \n",
    "\n",
    "    def approximateNearestNeighbors(self, key, n,):\n",
    "        \n",
    "        num_bands = int(self.num_bands)\n",
    "        selected_sigs = (self.signatures\n",
    "            .select(f\"{key}\")\n",
    "            .agg(F.collect_list(f\"{key}\"))\n",
    "            .rdd.map(lambda x: hash_bands(x[0], num_bands))\n",
    "            .flatMap(lambda x: x)\n",
    "            .map(lambda x: (x, ))\n",
    "            .toDF([\"buckets\"])\n",
    "        )\n",
    "\n",
    "        similar_docs = buckets.join(selected_sigs, \"buckets\")\n",
    "        doc_id = similar_docs.select(F.explode(\"doc_ids\").alias(\"doc_id\")).distinct().collect()\n",
    "        similar_docs = []\n",
    "\n",
    "        # TODO: Implement threading for parallel processing\n",
    "        for doc in doc_id:\n",
    "            jac_sim = self.jaccard_similarity(key, doc.doc_id)\n",
    "            if jac_sim > n:\n",
    "                similar_docs.append((doc.doc_id, jac_sim))\n",
    "        \n",
    "        return similar_docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"minHashLSH\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"This is a test document\",), (\"This document is another test document\",), (\"This is a test document\",), (\"Hello wordello \",), (\"Word Hello world\",), (\"hello\", )]\n",
    "df = spark.createDataFrame(data, [\"text\"])\n",
    "spark_lsh = SparkMinHashLSH(df)\n",
    "\n",
    "# Increase the number of bands and perms to decrease the sensitivity of the LSH, effectively reducing the number of false positives\n",
    "shingle_length = 2\n",
    "signature_length = 128\n",
    "num_bands = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+-----+-----+-----+-----+\n",
      "|shingles|    1|    2|    3|    4|    5|    6|\n",
      "+--------+-----+-----+-----+-----+-----+-----+\n",
      "|      ld|false|false|false|false| true|false|\n",
      "|       H|false|false|false|false| true|false|\n",
      "|      en| true| true| true|false|false|false|\n",
      "|       a| true| true| true|false|false|false|\n",
      "|      nt| true| true| true|false|false|false|\n",
      "|      t | true| true| true|false|false|false|\n",
      "|      st| true| true| true|false|false|false|\n",
      "|      oc| true| true| true|false|false|false|\n",
      "|      a | true|false| true|false|false|false|\n",
      "|      rl|false|false|false|false| true|false|\n",
      "+--------+-----+-----+-----+-----+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bool_vecs = spark_lsh.shingling(documents=df, k=shingle_length)\n",
    "bool_vecs.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+\n",
      "|  1|  2|  3|  4|  5|  6|\n",
      "+---+---+---+---+---+---+\n",
      "|  1| 21|  1| 11|  2| 11|\n",
      "|  7|  7|  7|120| 12|210|\n",
      "|  1|  1|  1| 13| 13| 17|\n",
      "|140|140|140|240|120|280|\n",
      "|  5|  5|  5|  2|  2|  2|\n",
      "| 13| 20| 13| 88|  6| 81|\n",
      "|  8|  8|  8|  0|  0|  0|\n",
      "|136|136|136|154|130|421|\n",
      "|  1|  2|  1| 25| 24| 73|\n",
      "| 97| 97| 97| 27| 35|105|\n",
      "+---+---+---+---+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigs = spark_lsh.minhashing(bool_vecs=bool_vecs, sig_length=signature_length)\n",
    "sigs.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|             buckets|doc_ids|\n",
      "+--------------------+-------+\n",
      "| 1368473407760555883| [1, 3]|\n",
      "|-1091538363657981302|    [5]|\n",
      "|-2359805777066111308|    [6]|\n",
      "|  789066639884840564|    [4]|\n",
      "| 5391101189165332098| [1, 3]|\n",
      "| 2975859297736561103|    [4]|\n",
      "|  528502270807337801| [1, 3]|\n",
      "| 2906227291019145655|    [6]|\n",
      "| 7728498595413907917|    [2]|\n",
      "|  472232209436246182|    [5]|\n",
      "+--------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buckets = spark_lsh.locality_sensitive_hashing(sigs=sigs, num_bands=num_bands)\n",
    "buckets.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1.0), (3, 1.0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_id = 1\n",
    "similarity_percentage = 0.6\n",
    "spark_lsh.approximateNearestNeighbors(doc_id, similarity_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shingling took 9.954345226287842 seconds\n",
      "Minhashing took 2.7686731815338135 seconds\n",
      "Locality Sensitive Hashing took 2.3187873363494873 seconds\n",
      "+--------------------+---------+\n",
      "|             buckets|  doc_ids|\n",
      "+--------------------+---------+\n",
      "| 3883364387212965600|[1, 2, 3]|\n",
      "|-8907760414718763540|   [1, 3]|\n",
      "| 1599942983381215927|   [1, 3]|\n",
      "| 3244284212049768524|   [5, 4]|\n",
      "|-8645571619998295187|      [6]|\n",
      "| 5646003302091366357|      [2]|\n",
      "| 8792398846230568615|      [6]|\n",
      "|  735482836909611368|      [5]|\n",
      "| 5763571626872289010|   [1, 3]|\n",
      "| 8389048192121911274|[1, 2, 3]|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buckets = spark_lsh.run(documents=df,k=2, bands=8, num_perms=16)\n",
    "buckets.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Phytoplasmas are insect-vectored bacteria that cause disease in a wide range of plant species. The increasing availability of molecular DNA analyses, expertise and additional methods in recent years has led to a proliferation of discoveries of phytoplasma-plant host associations and in the numbers of taxonomic groupings for phytoplasmas. The widespread use of common names based on the diseases with which they are associated, as well as separate phenetic and taxonomic systems for classifying phytoplasmas based on variation at the 16S rRNA-encoding gene, complicates interpretation of the literature. We explore this issue and related trends through a focus on Australian pathosystems, providing the first comprehensive compilation of information for this continent, covering the phytoplasmas, host plants, vectors and diseases. Of the 33 16Sr groups reported internationally, only groups I, II, III, X, XI and XII have been recorded in Australia and this highlights the need for ongoing biosecurity measures to prevent the introduction of additional pathogen groups. Many of the phytoplasmas reported in Australia have not been sufficiently well studied to assign them to 16Sr groups so it is likely that unrecognized groups and sub-groups are present. Wide host plant ranges are apparent among well studied phytoplasmas, with multiple crop and non-crop species infected by some. Disease management is further complicated by the fact that putative vectors have been identified for few phytoplasmas, especially in Australia. Despite rapid progress in recent years using molecular approaches, phytoplasmas remain the least well studied group of plant pathogens, making them a \"crouching tiger\" disease threat.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|Background: (-)-alpha-Bisabolol, also known as levomenol, is an unsaturated sesquiterpene alcohol that has mainly been used in pharmaceutical and cosmetic products due to its anti-inflammatory and skin-soothing properties. (-)-alpha-Bisabolol is currently manufactured mainly by steam-distillation of the essential oils extracted from the Brazilian candeia tree that is under threat because its natural habitat is constantly shrinking. Therefore, microbial production of (-)-alpha-bisabolol plays a key role in the development of its sustainable production from renewable feedstock. Results: Here, we created an Escherichia coli strain producing (-)-alpha-bisabolol at high titer and developed an in situ extraction method of (-)-alpha-bisabolol, using natural vegetable oils. We expressed a recently identified (-)-alpha-bisabolol synthase isolated from German chamomile (Matricaria recutita) (titer: 3 mg/L), converted the acetyl-CoA to mevalonate, using the biosynthetic mevalonate pathway (12.8 mg/L), and overexpressed farnesyl diphosphate synthase to efficiently supply the (-)-alpha-bisabolol precursor farnesyl diphosphate. Combinatorial expression of the exogenous mevalonate pathway and farnesyl diphosphate synthase enabled a dramatic increase in (-)-alpha-bisabolol production in the shake flask culture (80 mg/L) and 5 L bioreactor culture (342 mg/L) of engineered E. coli harboring (-)-alpha-bisabolol synthase. Fed-batch fermentation using a 50 L fermenter was conducted after optimizing culture conditions, resulting in efficient (-)-alpha-bisabolol production with a titer of 9.1 g/L. Moreover, a green, downstream extraction process using vegetable oils was developed for in situ extraction of (-)-alpha-bisabolol during fermentation and showed high yield recovery (>98%). Conclusions: The engineered E. coli strains and economically viable extraction process developed in this study will serve as promising platforms for further development of microbial production of (-)-alpha-bisabolol at large scale.                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|A universal feature of the replication of positive-strand RNA viruses is the association with intracellular membranes. Carnation Italian ringspot virus (CIRV) replication in plants occurs in vesicles derived from the mitochondrial outer membrane. The product encoded by CIRV ORF1, p36, is required for targeting the virus replication complex to the outer mitochondrial membrane both in plant and yeast cells. Here the yeast Saccharomyces cerevisiae was used as a model host to study the effect of CIRV p36 on cell survival and death. It was shown that p36 does not promote cell death, but decreases cell growth rate. In addition, p36 changed the nature of acetic acid-induced cell death in yeast by increasing the number of cells dying by necrosis with concomitant decrease of the number of cells dying by programmed cell death, as judged by measurements of phosphatidylserine externalization. The tight association of p36 to membranes was not affected by acetic acid treatment, thus confirming the peculiar and independent interaction of CIRV p36 with mitochondria in yeast. This work proved yeast as an invaluable model organism to study both the mitochondrial determinants of the type of cell death in response to stress and the molecular pathogenesis of (+)RNA viruses. (C) 2016 Elsevier Ireland Ltd. All rights reserved.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|1,2-Dichloropropane (1,2-DCP) and dichloromethane (DCM) are possible causative agents associated with the development of cholangiocarcinoma in employees working in printing plant in Osaka, Japan. However, few reports have demonstrated an association between these agents and cholangiocarcinoma in rodent carcinogenicity studies. Moreover, the combined effects of these compounds have not been fully elucidated. In the present study, we evaluated the in vivo mutagenicity of 1,2-DCP and DCM, alone or combined, in the livers of gpt delta rats. Six-week-old male F344 gpt delta rats were treated with 1,2-DCP, DCM or 1,2-DCP+DCM by oral administration for 4weeks at the dose (200mgkg(-1) body weight 1,2-DCP and 500mgkg(-1) body weight DCM) used in the carcinogenesis study performed by the National Toxicology Program. In vivo mutagenicity was analyzed by gpt mutation/Spi(-) assays in the livers of rats. In addition, gene and protein expression of CYP2E1 and GSTT1, the major enzymes responsible for the genotoxic effects of 1,2-DCP and DCM, were analyzed by quantitative polymerase chain reaction and western blotting. Gpt and Spi(-) mutation frequencies were not increased by 1,2-DCP and/or DCM in any group. Additionally, there were no significant changes in the gene and protein expression of CYP2E1 and GSTT1 in any group. These results indicated that 1,2-DCP, DCM and 1,2-DCP+DCM had no significant impact on mutagenicity in the livers of gpt delta rats under our experimental conditions. Copyright (c) 2016 John Wiley & Sons, Ltd.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|This paper presents the simulation results of a linear, fully integrated, two-stage digitally programmable 130 nm CMOS power amplifier (PA) operating at 2.4 GHz. Its power stage is composed of a set of amplifying cells which can be enabled or disabled independently by a digital control circuit. All seven operational modes are univocal in terms of 1 dB output compression point (OCP1dB), saturated output power (P-SAT) and power gain at 2.4 GHz. The lowest power mode achieves an 8.1 dBm P-SAT, a 13.5 dB power gain and consumes 171 mW DC power (P-DC) at an OCP1dB of 6 dBm, whereas the highest power mode reaches an 18.9 dBm P-SAT and a 21.1 dB power gain and consumes 415 mW P-DC at an OCP1dB of 18.2 dBm.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|The glycogen branching enzyme (EC 2.4.1.18), which catalyses the formation of alpha-1,6-glycosidic branch points in glycogen structure, is often used to enhance the nutritional value and quality of food and beverages. In order to be applicable in industries, enzymes that are stable and active at high temperature are much desired. Using genome mining, the nucleotide sequence of the branching enzyme gene (glgB) was extracted from the Geobacillus mahadia Geo-05 genome sequence provided by the Malaysia Genome Institute. The size of the gene is 2013 bp, and the theoretical molecular weight of the protein is 78.43 kDa. The gene sequence was then used to predict the thermostability, function and the three dimensional structure of the enzyme. The gene was cloned and overexpressed in E. coli to verify the predicted result experimentally. The purified enzyme was used to study the effect of temperature and pH on enzyme activity and stability, and the inhibitory effect by metal ion on enzyme activity. This thermostable glycogen branching enzyme was found to be most active at 55 degrees C, and the half-life at 60 degrees C and 70 degrees C was 24 h and 5 h, respectively. From this research, a thermostable glycogen branching enzyme was successfully isolated from Geobacillus mahadia Geo-05 by genome mining together with molecular biology technique.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|The microbial populations in the activated sludge of two Polish wastewater treatment plants (WWTPs) were identified and quantified using Illumina sequencing of 16S ribosomal RNA amplicons over a 2-year period. Their dynamics over time were compared to Danish WWTPs (data collected in previous studies by Center for Microbial Communities, Aalborg University). The bacterial communities in Polish and Danish WWTPs were similar to each other, but the microbial diversity in Polish WWTPs was lower. The dominant genera in Polish WWTPs were more abundant than in Danish WWTPs; 30 of them constituted more than half the of activated sludge community. Polish WWTPs showed a higher abundance of bacteria involved in nitrogen and chemical oxygen demand removal (Proteobacteria and Bacteroidetes), while polyphosphate-acculumating bacteria were the dominant bacterial group in Danish plants. The microbial community structures in the examined Polish WWTPs were relatively similar to each other and showed strong seasonal variations which are not normally observed in Danish WWTPs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|Background: Broad-range 16S rRNA PCR can be used for the detection and identification of bacteria from clinical specimens in patients for whom there is a high suspicion of infection and cultures are negative. The aims of this study were (1) to compare 16S rRNA PCR results with microbiolog ical culture results, (2) to assess the utility of 16S rRNA PCR with regard to antimicrobial therapy, and (3) to compare the yield of 16S rRNA PCR for different types of clinical specimen and to perform a cost analysis of the test. Methods: A retrospective study was performed on different clinical specimens which had 16S performed over 3 years (2012-2015). Standard microbiological cultures were performed on appropriate media, as per the laboratory protocol. Patient clinical and microbiological data were obtained from the electronic medical records and laboratory information system, respectively. 16S rRNA PCR was performed in a reference laboratory using a validated method for amplification and sequencing. The outcomes assessed were the performance of 16S rRNA PCR, change of antimicrobials (rationalization, cessation, or addition), and duration of therapy. Concordance of 16S rRNA PCR with bacterial cultures was also determined for tissue specimens. Results: Thirty-two patients were included in the study, for whom an equal number of specimens (n = 32) were sent for 16S rRNA PCR. 16S rRNA PCR could identify an organism in 10 of 32 cases (31.2%), of which seven were culture-positive and three were culture-negative. The sensitivity was 58% (confidence interval (CI) 28.59-83.5%) and specificity was 85% (CI 61.13-96%), with a positive predictive value of 70% (CI 35.3-91.9%) and negative predictive value of 77.2% (CI 54.17-91.3%). Antimicrobial therapy was rationalized after 16S rRNA PCR results in five patients (15.6%) and was ceased in four based on negative results (12.5%). Overall the 16S rRNA PCR result had an impact on antimicrobial therapy in 28% of patients (9/32). The highest concordance of 16S rRNA PCR with bacterial culture was found for heart valve tissue (80%), followed by joint fluid/tissue (50%). Conclusions: Despite the low diagnostic yield, results of 16S rRNA PCR can still have a significant impact on patient management due to rationalization or cessation of the antimicrobial therapy. The yield of 16S rRNA PCR was highest for heart valves. (C) 2017 The Author(s). Published by Elsevier Ltd on behalf of International Society for Infectious Diseases.|\n",
      "|Brinjal little leaf (BLL) is a widespread disease of phytoplasma etiology in India that induces severe economic losses. Surveys were conducted in eight brinjalgrowing states of India during July 2014 to September 2015 and eighteen BLL samples showing little leaf, phyllody and witches' broom symptoms were collected for phytoplasma identification. Presence of phytoplasmas was confirmed in all the eighteen BLL samples using polymerase chain reaction with phytoplasma-specific primer pairs (P1/P6, R16F2n/R16R2). Pair wise sequence comparison and phylogenetic relationship of 16S rRNA gene sequences of BLL phytoplasma strains confirmed that sixteen out of eighteen BLL strains belonged to clover proliferation phytoplasma (16SrVI) group and two BLL strains (GKP-A and GKP-B) from Gorakhpur, Uttar Pradesh, were classified under 16SrII group. Further virtual RFLP analysis of 16S rDNA sequences allowed finer classification of BLL strains into 16SrII-D and 16SrVI-D subgroups. BLL phytoplasma strains belonging to 16SrVI-D subgroup were found as the most widespread phytoplasma strains associated with BLL disease in India. 16SrVI-D subgroup phytoplasma association with two symptomatic weed species viz. Cannabis sativa subsp. sativa at Noida, Uttar Pradesh and Portulaca oleracea at IARI fields, New Delhi was also confirmed by nested PCR assays with similar set of phytoplasma-specific primers, pairwise 16S rDNA sequence comparison, phylogeny and virtual RFLP analysis. Out of five identified leafhopper species from BLL-infected fields at Noida, Uttar Pradesh and Delhi, only Hishimonas phycitis was identified as carrier and natural vector of 16SrVI-D subgroup of phytoplasmas by nested PCR assays, sequence comparison, phylogeny, virtual RFLP analysis and transmission assays.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Surveys for the Cote d'Ivoire lethal yellowing (CILY) phytoplasma were conducted in eight severely CILY-affected villages of Grand-Lahou in 2015. Leaves, inflorescences and trunk borings were collected from coconut palms showing CILY symptoms and from symptomless trees. Total DNA was extracted from these samples and tested by nested polymerase chain reaction/RFLP and sequence analysis of the 16S rRNA, ribosomal protein (rp) and the translocation protein (secA) genes. The CILY phytoplasma was detected in 82.9% of the symptom-bearing palms collected from all the surveyed villages and from all the plant parts. Trunk borings were recommended as the most suitable plant tissue type for sampling. Results indicate that the CILY phytoplasma may have a westward spread to other coconut-growing areas of Grand-Lahou. CILY phytoplasma strains infecting coconut palms in the western region of Grand-Lahou exhibited unique single nucleotide polymorphisms on the rp sequence compared to the strains from the eastern region. Moreover, single nucleotide polymorphisms on the SecA sequence distinguished the CILY phytoplasma from the Cape St. Paul Wilt Disease phytoplasma in Ghana, and the Lethal Yellowing phytoplasma in Mozambique.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_web_of_science_5736 = spark.read.text(\"WebOfScience-5736.txt\", lineSep=\"\\r\\n\")\n",
    "df_web_of_science_5736.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shingling took 2.2737371921539307 seconds\n",
      "Minhashing took 18.19093418121338 seconds\n",
      "Locality Sensitive Hashing took 16.542454481124878 seconds\n",
      "+--------------------+--------------------+\n",
      "|             buckets|             doc_ids|\n",
      "+--------------------+--------------------+\n",
      "| 8483113381247461651|                [82]|\n",
      "| 7571275679603839956|    [85, 53, 58, 76]|\n",
      "|-8725076169812727400|[66, 45, 37, 74, ...|\n",
      "|  626009420018547013|[30, 82, 38, 74, ...|\n",
      "| -445724793277990459|[81, 34, 31, 38, ...|\n",
      "| 6917473940299867721|[15, 66, 60, 1, 1...|\n",
      "|-2534679458921155598|                 [7]|\n",
      "|-4630518706558724794|            [66, 27]|\n",
      "| 3228296493935762207|     [7, 58, 94, 92]|\n",
      "|-2433500303205739586|     [63, 53, 6, 69]|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wos_bucket = spark_lsh.run(documents=df_web_of_science_5736.limit(100), k=20, bands=8, num_perms=32)\n",
    "wos_bucket.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_lsh.approximateNearestNeighbors(60, 0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
